{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised machine learning\n",
    "\n",
    "\n",
    "\n",
    "This section is partially inspired by the following Reference: http://cs229.stanford.edu/notes/cs229-notes1.pdf\n",
    "\n",
    "- **Features** (input variables) $x^{(i)}\\in \\mathbb{X}$ \n",
    "- **Target** (output we are trying to predict) $y^{(i)} \\in \\mathbb{Y}$\n",
    "\n",
    "$(x^{(i)},y^{(i)})$ is a **training example**\n",
    "\n",
    "$\\{(x^{(i)},y^{(i)}); i = 1,...,m\\}$ is the **training set**\n",
    "\n",
    "The goal of supervised learning is to learn a function $h: \\mathbb{X}\\mapsto\\mathbb{Y}$, called the hypothesis, so that $h(x)$ is a good \n",
    "predictor of the corresponding $y$.\n",
    "\n",
    "- **Regression** correspond to the case where $y$ is a continuous variable\n",
    "- **Classification** correspond to the case where $y$ can only take a small number of discrete values\n",
    "\n",
    "Examples: \n",
    "- Univariate Linear Regression: $h_w(x) = w_0+w_1x$,  with $\\mathbb{X} = \\mathbb{Y} = \\mathbb{R}$\n",
    "- Multivariate Linear Regression: $$h_w(x) = w_0+w_1x_1 + ... + w_nx_n = \\sum_{i=0}^{n}w_ix_i = w^Tx,$$\n",
    "with $\\mathbb{Y} = \\mathbb{R}$ and $\\mathbb{X} = \\mathbb{R^n}$.\n",
    "Here $w_0$ is the intercept with the convention that $x_0=1$ to simplify notation.\n",
    "\n",
    "\n",
    "\n",
    "## Binary Classification with Logistic Regression\n",
    "\n",
    "- $y$ can take only two values, 0 or 1. For example, if $y$ is the sentiment associated with the tweet,\n",
    "$y=1$ if the tweet is \"positive\" and $y=0$ is the tweet is \"negative\".\n",
    "\n",
    "- $x^{(i)}$ represents the features of a tweet. For example the presence or absence of certain words.\n",
    "\n",
    "- $y^{(i)}$ is the **label** of the training example represented by $x^{(i)}$.\n",
    "\n",
    "\n",
    "Since $y\\in\\{0,1\\}$ we want to limit $h_w(x)$ between $[0,1]$.\n",
    "\n",
    "The **Logistic regression** consists of choosing $h_w(x)$ as\n",
    "\n",
    "$$\n",
    "h_w(x) = \\frac{1}{1+e^{-w^Tx}}\n",
    "$$\n",
    "\n",
    "where $w^Tx = \\sum_{i=0}^{n}w_ix_i$ and $h_w(x) = g(w^Tx)$ with\n",
    "\n",
    "$$\n",
    "g(x)=\\frac{1}{1+e^{-x}}.\n",
    "$$\n",
    "\n",
    "$g(x)$ is the **logistic function** or **sigmoid function**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.linspace(-10,10)\n",
    "y = 1/(1+np.exp(-x))\n",
    "\n",
    "p = plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $g(x)\\rightarrow 1$ for $x\\rightarrow\\infty$\n",
    "- $g(x)\\rightarrow 0$ for $x\\rightarrow -\\infty$\n",
    "- $g(0) = 1/2$\n",
    "\n",
    "Finally, to go from the regression to the classification, we can simply apply the following condition:\n",
    "\n",
    "$$\n",
    "y=\\left\\{\n",
    "  \\begin{array}{@{}ll@{}}\n",
    "    1, & \\text{if}\\ h_w(x)>=1/2 \\\\\n",
    "    0, & \\text{otherwise}\n",
    "  \\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Let's clarify the notation. We have **$m$ training samples** and **$n$ features**, our training examples can be represented by a **$m$-by-$n$ matrix** $\\underline{\\underline{X}}=(x_{ij})$ ($m$-by-$n+1$, if we include the intercept term) that contains the training examples, $x^{(i)}$, in its rows.\n",
    "\n",
    "The target values of the training set can be represented as a $m$-dimensional vector $\\underline{y}$ and the parameters \n",
    "of our model as\n",
    "a $n$-dimensional vector $\\underline{w}$ ($n+1$ if we take into account the intercept).\n",
    "\n",
    "Now, for a given training example $x^{(i)}$, the function that we want to learn (or fit) can be written:\n",
    "\n",
    "$$\n",
    "h_\\underline{w}(x^{(i)}) = \\frac{1}{1+e^{-\\sum_{j=0}^n w_j x_{ij}}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood of the model\n",
    "\n",
    "How to find the parameters, also called *weights*, $\\underline{w}$ that best fit our training data?\n",
    "We want to find the weights $\\underline{w}$ that maximize the likelihood of observing the target $\\underline{y}$ given the observed features $\\underline{\\underline{X}}$.\n",
    "We need a probabilistic model that gives us the probability of observing the value $y^{(i)}$ given the features $x^{(i)}$.\n",
    "\n",
    "The function $h_\\underline{w}(x^{(i)})$ can be used precisely for that:\n",
    "\n",
    "$$\n",
    "P(y^{(i)}=1|x^{(i)};\\underline{w}) = h_\\underline{w}(x^{(i)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(y^{(i)}=0|x^{(i)};\\underline{w}) = 1 - h_\\underline{w}(x^{(i)})\n",
    "$$\n",
    "\n",
    "\n",
    "we can write is more compactly as:\n",
    "\n",
    "$$\n",
    "P(y^{(i)}|x^{(i)};\\underline{w}) = (h_\\underline{w}(x^{(i)}))^{y^{(i)}} ( 1 - h_\\underline{w}(x^{(i)}))^{1-y^{(i)}}\n",
    "$$\n",
    "where $y^{(i)}\\in{0,1}$\n",
    "\n",
    "\n",
    "We see that $y^{(i)}$ is a random variable following a Bernouilli distribution with expectation $h_\\underline{w}(x^{(i)})$.\n",
    "\n",
    "\n",
    "\n",
    "The **Likelihood function** of a statistical model is defined as:\n",
    "$$\n",
    "\\mathcal{L}(\\underline{w}) = \\mathcal{L}(\\underline{w};\\underline{\\underline{X}},\\underline{y}) = P(\\underline{y}|\\underline{\\underline{X}};\\underline{w}).\n",
    "$$\n",
    "\n",
    "The likelihood takes into account all the $m$ training samples of our training dataset and estimates the likelihood \n",
    "of observing $\\underline{y}$ given $\\underline{\\underline{X}}$ and $\\underline{w}$. Assuming that the $m$ training examples were generated independently, we can write:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\underline{w}) = P(\\underline{y}|\\underline{\\underline{X}};\\underline{w}) = \\prod_{i=1}^m P(y^{(i)}|x^{(i)};\\underline{w}) = \\prod_{i=1}^m (h_\\underline{w}(x^{(i)}))^{y^{(i)}} ( 1 - h_\\underline{w}(x^{(i)}))^{1-y^{(i)}}.\n",
    "$$\n",
    "\n",
    "This is the function that we want to maximize. It is usually much simpler to maximize the logarithm of this function, which is equivalent.\n",
    "\n",
    "$$\n",
    "l(\\underline{w}) = \\log\\mathcal{L}(\\underline{w}) = \\sum_{i=1}^{m} \\left(y^{(i)} \\log h_\\underline{w}(x^{(i)}) + (1- y^{(i)})\\log(1- h_\\underline{w}(x^{(i)})) \\right)\n",
    "$$\n",
    "\n",
    "### Loss function and linear models\n",
    "\n",
    "An other way of formulating this problem is by defining a Loss function $L\\left(y^{(i)}, f(x^{(i)})\\right)$ such that:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{m} L\\left(y^{(i)}, f(x^{(i)})\\right) = - l(\\underline{w}).\n",
    "$$\n",
    "\n",
    "And now the problem consists of minimizing $\\sum_{i=1}^{m} L\\left(y^{(i)}, f(x^{(i)})\\right)$ over all the possible values of $\\underline{w}$.\n",
    "\n",
    "Using the definition of $h_\\underline{w}(x^{(i)})$ you can show that $L$ can be written as:\n",
    "$$\n",
    "L\\left(y^{(i)}=1, f(x^{(i)})\\right) = \\log_2\\left(1+e^{-f(x^{(i)})}\\right)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "L\\left(y^{(i)}=0, f(x^{(i)})\\right) = \\log_2\\left(1+e^{-f(x^{(i)})}\\right) - \\log_2\\left(e^{-f(x^{(i)})}\\right)\n",
    "$$\n",
    "\n",
    "where $f(x^{(i)}) = \\sum_{j=0}^n w_j x_{ij}$ is called the **decision function**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fx = np.linspace(-5,5)\n",
    "Ly1 = np.log2(1+np.exp(-fx))\n",
    "Ly0 = np.log2(1+np.exp(-fx)) - np.log2(np.exp(-fx))\n",
    "\n",
    "p = plt.plot(fx,Ly1,label='L(1,f(x))')\n",
    "p = plt.plot(fx,Ly0,label='L(0,f(x))')\n",
    "plt.xlabel('f(x)')\n",
    "plt.ylabel('L')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that although the loss function is not linear, the decision function is a **linear function of the weights and features**. This is why the Logistic regression is part called a **linear model**.\n",
    "\n",
    "Other linear models are defined by different loss functions. For example:\n",
    "- Perceptron: $L \\left(y^{(i)}, f(x^{(i)})\\right) = \\max(0, -y^{(i)}\\cdot f(x^{(i)}))$\n",
    "- Hinge-loss (soft-margin Support vector machine (SVM) classification): $L \\left(y^{(i)}, f(x^{(i)})\\right) = \\max(0, 1-y^{(i)}\\cdot f(x^{(i)}))$\n",
    "\n",
    "See http://scikit-learn.org/stable/modules/sgd.html for more examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fx = np.linspace(-5,5, 200)\n",
    "Logit = np.log2(1+np.exp(-fx))\n",
    "Percep = np.maximum(0,- fx) \n",
    "Hinge = np.maximum(0, 1- fx)\n",
    "ZeroOne = np.ones(fx.size)\n",
    "ZeroOne[fx>=0] = 0\n",
    "\n",
    "p = plt.plot(fx,Logit,label='Logistic Regression')\n",
    "p = plt.plot(fx,Percep,label='Perceptron')\n",
    "p = plt.plot(fx,Hinge,label='Hinge-loss')\n",
    "p = plt.plot(fx,ZeroOne,label='Zero-One loss')\n",
    "plt.xlabel('f(x)')\n",
    "plt.ylabel('L')\n",
    "plt.legend()\n",
    "ylims = plt.ylim((0,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tweet sentiment analysis\n",
    "\n",
    "dataset: http://help.sentiment140.com/for-students/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a DataFrame with each tweet using pandas\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "def getTweetID(tweet):\n",
    "    \"\"\" If properly included, get the ID of the tweet \"\"\"\n",
    "    return tweet.get('id')\n",
    "    \n",
    "def getUserIDandScreenName(tweet):\n",
    "    \"\"\" If properly included, get the tweet \n",
    "        user ID and Screen Name \"\"\"\n",
    "    user = tweet.get('user')\n",
    "    if user is not None:\n",
    "        uid = user.get('id')\n",
    "        screen_name = user.get('screen_name')\n",
    "        return uid, screen_name\n",
    "    else:\n",
    "        return (None, None)\n",
    "    \n",
    "\n",
    "    \n",
    "filename = 'trumpTweets.txt'\n",
    "\n",
    "# create a list of dictionaries with the data that interests us\n",
    "tweet_data_list = []\n",
    "with open(filename, 'r') as fopen:\n",
    "    # each line correspond to a tweet\n",
    "    for line in fopen:\n",
    "        tweet = json.loads(line)\n",
    "        tweet_data_list.append({'tweet_id' : getTweetID(tweet),\n",
    "                       'user_id' : getUserIDandScreenName(tweet)[0],\n",
    "                       'text' : tweet.get('text')})\n",
    "\n",
    "# put everything in a dataframe\n",
    "tweet_df = pd.DataFrame.from_dict(tweet_data_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10931, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @FoxNews: Trump Claims Top Dem Told Him He'...</td>\n",
       "      <td>8.500716e+17</td>\n",
       "      <td>8.396139e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#Trump doesn't care about governing. But he su...</td>\n",
       "      <td>8.500716e+17</td>\n",
       "      <td>2.238084e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Don’t miss today’s Federalist Radio on Syria, ...</td>\n",
       "      <td>8.500716e+17</td>\n",
       "      <td>1.408004e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump is reportedly weighing up military strik...</td>\n",
       "      <td>8.500716e+17</td>\n",
       "      <td>1.177421e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Schadenfreude... #obama gets a taste of his tr...</td>\n",
       "      <td>8.500716e+17</td>\n",
       "      <td>1.177525e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT @JoinLauraNow: This ENTIRE #Syria killing o...</td>\n",
       "      <td>8.500716e+17</td>\n",
       "      <td>3.938447e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @DylanByers: Who wants to tell him? https:/...</td>\n",
       "      <td>8.500716e+17</td>\n",
       "      <td>8.195901e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>$ATRM get 14 days #free #premium access on the...</td>\n",
       "      <td>8.500716e+17</td>\n",
       "      <td>8.056616e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RT @WilliamTurton: So Trump found out about th...</td>\n",
       "      <td>8.500716e+17</td>\n",
       "      <td>1.585515e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RT @ian_sager: In the meantime, this is how fo...</td>\n",
       "      <td>8.500716e+17</td>\n",
       "      <td>1.821638e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text      tweet_id  \\\n",
       "0  RT @FoxNews: Trump Claims Top Dem Told Him He'...  8.500716e+17   \n",
       "1  #Trump doesn't care about governing. But he su...  8.500716e+17   \n",
       "2  Don’t miss today’s Federalist Radio on Syria, ...  8.500716e+17   \n",
       "3  Trump is reportedly weighing up military strik...  8.500716e+17   \n",
       "4  Schadenfreude... #obama gets a taste of his tr...  8.500716e+17   \n",
       "5  RT @JoinLauraNow: This ENTIRE #Syria killing o...  8.500716e+17   \n",
       "6  RT @DylanByers: Who wants to tell him? https:/...  8.500716e+17   \n",
       "7  $ATRM get 14 days #free #premium access on the...  8.500716e+17   \n",
       "8  RT @WilliamTurton: So Trump found out about th...  8.500716e+17   \n",
       "9  RT @ian_sager: In the meantime, this is how fo...  8.500716e+17   \n",
       "\n",
       "        user_id  \n",
       "0  8.396139e+17  \n",
       "1  2.238084e+09  \n",
       "2  1.408004e+09  \n",
       "3  1.177421e+09  \n",
       "4  1.177525e+07  \n",
       "5  3.938447e+08  \n",
       "6  8.195901e+17  \n",
       "7  8.056616e+17  \n",
       "8  1.585515e+07  \n",
       "9  1.821638e+08  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the first 10 rows\n",
    "tweet_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the logistic regression classifier\n",
    "https://www.dropbox.com/s/09rw6a85f7ezk31/sklearn_SGDLogReg_.pickle.zip?dl=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the classifier is saved in a \"pickle\" file\n",
    "import pickle\n",
    "\n",
    "with open('sklearn_SGDLogReg_.pickle', 'rb') as fopen:\n",
    "    classifier = pickle.load(fopen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label_inv_mapper': {0: 'neg', 1: 'pos'},\n",
       " 'label_mapper': {'neg': 0, 'pos': 1},\n",
       " 'sklearn_pipeline': Pipeline(steps=[('feat_vectorizer', DictVectorizer(dtype=<class 'numpy.int8'>, separator='=', sort=False,\n",
       "         sparse=True)), ('classifier', SGDClassifier(alpha=7.847599703514622e-06, average=False, class_weight=None,\n",
       "        epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "        learning_rate='optimal', loss='log', n_iter=10, n_jobs=1,\n",
       "        penalty='l2', power_t=0.5, random_state=42, shuffle=True, verbose=0,\n",
       "        warm_start=False))])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cls = classifier['sklearn_pipeline']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feat_vectorizer', DictVectorizer(dtype=<class 'numpy.int8'>, separator='=', sort=False,\n",
       "        sparse=True)), ('classifier', SGDClassifier(alpha=7.847599703514622e-06, average=False, class_weight=None,\n",
       "       epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', n_iter=10, n_jobs=1,\n",
       "       penalty='l2', power_t=0.5, random_state=42, shuffle=True, verbose=0,\n",
       "       warm_start=False))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cls.get_params\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [graphtool_compile]",
   "language": "python",
   "name": "Python [graphtool_compile]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
