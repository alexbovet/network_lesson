{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tweet sentiment analysis\n",
    "\n",
    "In this section we will see how to extract features from tweets and use a classifier to classify the tweet as positive or negative.\n",
    "\n",
    "We will use a pandas DataFrames (http://pandas.pydata.org/) to store tweets and process them.\n",
    "Pandas DataFrames are very powerful python data-structures, like excel spreadsheets with the power of python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's create a DataFrame with each tweet using pandas\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def getTweetID(tweet):\n",
    "    \"\"\" If properly included, get the ID of the tweet \"\"\"\n",
    "    return tweet.get('id')\n",
    "    \n",
    "def getUserIDandScreenName(tweet):\n",
    "    \"\"\" If properly included, get the tweet \n",
    "        user ID and Screen Name \"\"\"\n",
    "    user = tweet.get('user')\n",
    "    if user is not None:\n",
    "        uid = user.get('id')\n",
    "        screen_name = user.get('screen_name')\n",
    "        return uid, screen_name\n",
    "    else:\n",
    "        return (None, None)\n",
    "    \n",
    "\n",
    "    \n",
    "filename = 'tweets.txt'\n",
    "\n",
    "# create a list of dictionaries with the data that interests us\n",
    "tweet_data_list = []\n",
    "with open(filename, 'r') as fopen:\n",
    "    # each line correspond to a tweet\n",
    "    for line in fopen:\n",
    "        if line != '\\n':\n",
    "            tweet = json.loads(line.strip('\\n'))\n",
    "            tweet_id = getTweetID(tweet)\n",
    "            user_id = getUserIDandScreenName(tweet)[0]\n",
    "            text = tweet.get('text')\n",
    "            if tweet_id is not None:\n",
    "                tweet_data_list.append({'tweet_id' : tweet_id,\n",
    "                           'user_id' : user_id,\n",
    "                           'text' : text})\n",
    "\n",
    "# put everything in a dataframe\n",
    "tweet_df = pd.DataFrame.from_dict(tweet_data_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(tweet_df.shape)\n",
    "print(tweet_df.columns)\n",
    "\n",
    "#print 5 first element of one of the column\n",
    "print(tweet_df.text.iloc[:5])\n",
    "# or\n",
    "print(tweet_df['text'].iloc[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show the first 10 rows\n",
    "tweet_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from the tweets\n",
    "\n",
    "#### 1) Tokenize the tweet in a list of words\n",
    "\n",
    "This part uses concepts from [Naltural Langage Processing](https://en.wikipedia.org/wiki/Natural_language_processing).\n",
    "We will use a tweet tokenizer I built based on TweetTokenizer from NLTK (http://www.nltk.org/).\n",
    "You can see how it works by opening the file TwSentiment.py. The goal is to process any tweets and extract a list of words taking into account usernames, hashtags, urls, emoticons and all the informal text we can find in tweets. We also want to reduce the number of features by doing some transformations such as putting all the words in lower cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from TwSentiment import CustomTweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = CustomTweetTokenizer(preserve_case=False, # keep Upper cases\n",
    "                                 reduce_len=True, # reduce repetition of letter to a maximum of three\n",
    "                                 strip_handles=False, # remove usernames (@mentions)\n",
    "                                 normalize_usernames=True, # replace all mentions to \"@USER\"\n",
    "                                 normalize_urls=True, # replace all urls to \"URL\"\n",
    "                                 keep_allupper=True) # keep upercase for words that are all in uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example\n",
    "tweet_df.text.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer.tokenize(tweet_df.text.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# other examples\n",
    "tokenizer.tokenize('Hey! This is SO cooooooooooooooooool! :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer.tokenize('Hey! This is so cooooooool! :)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Define the features that will represent the tweet\n",
    "We will use the occurrence of words and pair of words (bigrams) as features.\n",
    "\n",
    "This corresponds to a bag-of-words representation (https://en.wikipedia.org/wiki/Bag-of-words_model): we just count each words (or [n-grams](https://en.wikipedia.org/wiki/N-gram)) without taking account their order. For document classification, the frequency of occurence of each words is usually taken as a feature. In the case of tweets, they are so short that we can just count each words once.\n",
    "\n",
    "Using pair of words allows to capture some of the context in which each words appear. This helps capturing the correct meaning of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from TwSentiment import bag_of_words_and_bigrams\n",
    "\n",
    "# this will return a dictionary of features,\n",
    "# we just list the features present in this tweet\n",
    "bag_of_words_and_bigrams(tokenizer.tokenize(tweet_df.text.iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the logistic regression classifier\n",
    "\n",
    "https://www.dropbox.com/s/09rw6a85f7ezk31/sklearn_SGDLogReg_.pickle.zip?dl=1\n",
    "\n",
    "I trained this classifier on this dataset: http://help.sentiment140.com/for-students/, following the approach from this paper: http://cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf\n",
    "\n",
    "This is a set of 14 million tweets with emoticons. Tweets containing \"sad\" emoticons (7 million) are considered negative and tweets with \"happy\" emoticons (7 million) are considered positive.\n",
    "\n",
    "I used a Logistic Regression classifier with L2 regularization that I optimized with a 10 fold cross-validation using $F_1$ score as a metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the classifier is saved in a \"pickle\" file\n",
    "import pickle\n",
    "\n",
    "with open('sklearn_SGDLogReg_.pickle', 'rb') as fopen:\n",
    "    classifier_dict = pickle.load(fopen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# classifier_dict contain the classifier and label mappers\n",
    "# that I added so that we remember how the classes are \n",
    "# encoded\n",
    "classifier_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier is in fact contained in a pipeline.\n",
    "A sklearn pipeline allows to assemble several transformation of your data (http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipline = classifier_dict['sklearn_pipeline']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In our case we have two steps: \n",
    "\n",
    "- Vectorize the textual features (using http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html)\n",
    "- Classify the vectorized features (using http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipline.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this the step that will transform a list of textual features to a vector of zeros and ones\n",
    "dict_vect = pipline.steps[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_vect.feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of features\n",
    "len(dict_vect.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a little example\n",
    "text = 'Hi all, I am very happy today'\n",
    "# first tokenize\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "# list features\n",
    "features = bag_of_words_and_bigrams(tokens)\n",
    "print(features)\n",
    "\n",
    "# vectorize features\n",
    "X = dict_vect.transform(features)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X is a special kind of numpy array. beacause it is extremely sparse\n",
    "# it can be encoded to take less space in memory\n",
    "# if we want to see it fully, we can use .toarray()\n",
    "\n",
    "# number of non-zero values in X:\n",
    "X.toarray().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapping between the list of features and the vector of zeros and ones is done when you train the pipeline with its `.fit` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Classifing the tweet\n",
    "Now that we have vector representing the presence of features in a tweet, we can apply our logistic regression classifier to compute the probability that a tweet belong to the \"sad\" or \"happy\" category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = pipline.steps[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# access the weights of the logistic regression\n",
    "classifier.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we have as many weights as features\n",
    "classifier.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plus the intrecept \n",
    "classifier.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's check the weight associated with a given feature\n",
    "x = dict_vect.transform({('sad'): True})\n",
    "_, ind = np.where(x.todense())\n",
    "print(classifier.coef_[0,ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = dict_vect.transform({('good'): True})\n",
    "_, ind = np.where(x.todense())\n",
    "print(classifier.coef_[0,ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = dict_vect.transform({('not', 'sad'): True})\n",
    "_, ind = np.where(x.todense())\n",
    "print(classifier.coef_[0,ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find the probability for a specific tweet\n",
    "classifier.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sklearn pipeline to group the two last steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipline.predict_proba(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see to numbers, the first one is the probability of the tweet being sad, the second one is the probability of the tweet being happy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# note that:\n",
    "pipline.predict_proba(features).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together:\n",
    "\n",
    "We will use the class `TweetClassifier` from TwSentiment.py that puts together this process for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from TwSentiment import TweetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twClassifier = TweetClassifier(pipline,\n",
    "                              tokenizer=tokenizer,\n",
    "                              feature_extractor=bag_of_words_and_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example\n",
    "text = 'Hi all, I am very happy today'\n",
    "print(twClassifier.classify_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the classify text method also accepts a list of text as input\n",
    "print(twClassifier.classify_text(['great day today!', \"bad day today...\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# you'll see that if the sentence becomes more complicated, \n",
    "# the classifier is not as accurate\n",
    "print(twClassifier.classify_text([\"I am not sad\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(twClassifier.classify_text([\"I am not bad\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can now classify our tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emo_clas, prob = twClassifier.classify_text(tweet_df.text.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add the result to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_df['pos_class'] = (emo_clas == 'pos')\n",
    "tweet_df['pos_prob'] = prob[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the distribution of probability\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "h = plt.hist(tweet_df.pos_prob, bins=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to classify users based on the class of their tweets.\n",
    "Pandas allows to easily group tweets per users using the [groupy](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html) method of DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_group = tweet_df.groupby('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(user_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's look at one of the group\n",
    "groups = user_group.groups\n",
    "uid = list(groups.keys())[5]\n",
    "user_group.get_group(uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we need to make a function that takes the dataframe of tweets grouped by users and return the class of the users\n",
    "def get_user_emo(group):\n",
    "    num_pos = group.pos_class.sum()\n",
    "    num_tweets = group.pos_class.size\n",
    "    if num_pos/num_tweets > 0.5:\n",
    "        return 'pos'\n",
    "    elif num_pos/num_tweets < 0.5:\n",
    "        return 'neg'\n",
    "    else:\n",
    "        return 'NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply the function to each group\n",
    "user_df = user_group.apply(get_user_emo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a pandas Series where the index are the user_id\n",
    "user_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add this information to the graph we created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.read_graphml('twitter_lcc.graphml', node_type=int)\n",
    "\n",
    "for n in G.nodes_iter():\n",
    "    if n in user_df.index:\n",
    "        # here we look at the value of the user_df series at the position where the index \n",
    "        # is equal to the user_id of the node\n",
    "        G.node[n]['emotion'] = user_df.loc[user_df.index == n].values[0]\n",
    "        \n",
    "#we can also add the emotion associated with tweets to the edges of the graph\n",
    "for u,v, tweet_id in G.edges_iter(data='tweet_id'):\n",
    "    if tweet_df.tweet_id.isin([tweet_id]).any():\n",
    "        G.edge[u][v]['pos_class'] = int(tweet_df.loc[tweet_df.tweet_id == tweet_id].pos_class.values[0])\n",
    "        G.edge[u][v]['pos_prob'] = float(tweet_df.loc[tweet_df.tweet_id == tweet_id].pos_prob.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we have added an attribute 'emotion' to the nodes\n",
    "G.node[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G.edge[u][v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the graph to open it with Gephi\n",
    "nx.write_graphml(G, 'twitter_lcc_emo.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can now open this file with [Gephi](https://gephi.org/) to vizualize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example where the size of nodes is proportional to their in-degree, their color indicate their out-degree (from white to dark green) and the color of edges indicates the probability of the tweet carrying an \"happy\" sentiment (blue = sad, orange = happy).\n",
    "\n",
    "<img src=\"emo_network.png\" style=\"width: 1024px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very inclomplete list of references to go further:\n",
    "\n",
    "- Perkins, J. Python 3 Text Processing With NLTK 3 Cookbook. Python 3 Text Processing With NLTK 3 Cookbook (2014).\n",
    "- Hastie, T., Tibshirani, R. & Friedman, J. The Elements of Statistical Learning. Elements 1, (Springer New York, 2009).\n",
    "- Serrano-Guerrero, J., Olivas, J. A., Romero, F. P. & Herrera-Viedma, E. Sentiment analysis: A review and comparative analysis of web services. Inf. Sci. (Ny). 311, 18–38 (2015).\n",
    "- Go, A., Bhayani, R. & Huang, L. Twitter Sentiment Classification using Distant Supervision. Tech. Rep. 150, 1–6 (2009).\n",
    "- O’Connor, B., Balasubramanyan, R., Routledge, B. R. & Smith, N. a. From tweets to polls: Linking text sentiment to public opinion time series. Proc. 4h Int. AAAI Conf. Weblogs Soc. Media 122–129 (2010)-\n",
    "- Hannak, A., Anderson, E., Barrett, L. F., Lehmann, S., Mislove, A. & Riedewald, M. Tweetin’ in the Rain: Exploring societal-scale effects of weather on mood. in Proc. of the 6th International AAAI Conference on Weblogs and Social Media 479–482 (2012).\n",
    "- Jungherr, A., Schoen, H., Posegga, O. & Ju rgens, P. Digital Trace Data in the Study of Public Opinion: An Indicator of Attention Toward Politics Rather Than Political Support. Soc. Sci. Comput. Rev. 894439316631043 (2016).\n",
    "- Gayo-Avello, D. A Meta-Analysis of State-of-the-Art Electoral Prediction From Twitter Data. Soc. Sci. Comput. Rev. 31, 649–679 (2013).\n",
    "- Ceron, A., Curini, L. & Iacus, S. M. ISA: A fast, scalable and accurate algorithm for sentiment analysis of social media content. Inf. Sci. (Ny). 367–368, 105–124 (2016).\n",
    "- Bohannon, J. The pulse of the people. Science (80). 355, 470–472 (2017).\n",
    "- Bovet, A. Morone, F. & Makse, H.A. Validation of Twitter opinion trends with national polling aggregates: Hillary Clinton vs Donald Trump. arXiv:1610.01587 (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
